{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"2wbLfKVB1Qss"},"source":["## Importing the necessary modules"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1663139471533,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"k0U8RAiQgfVD","outputId":"5a65cfb0-232b-46f8-b9d5-1da76b225044"},"outputs":[],"source":["from json import loads\n","from keras import Sequential\n","from keras.layers import Dense, Dropout, LeakyReLU, Softmax\n","from keras.optimizers import Adam\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from numpy import array\n","from random import choice, shuffle\n","from string import punctuation\n","'''\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')\n","'''\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = loads(open(\"dataset/intents.json\").read())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tB3oHTpr1Uz_"},"source":["## Using NLP to clean the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","\n","# Each list to create\n","words = []\n","classes = []\n","doc_X = []\n","doc_y = []\n","\n","'''Looping through all the intents and tokenizing each patterns\n","and appending tokens to words, the patterns \n","and the associated tag to their associated list'''\n","for intent in data[\"intents\"]:\n","    for pattern in intent[\"patterns\"]:\n","        tokens = word_tokenize(pattern)\n","        words.extend(tokens)\n","        doc_X.append(pattern)\n","        doc_y.append(intent[\"tag\"])\n","    # add the tag to the classes if it's not there already\n","    if intent[\"tag\"] not in classes:\n","        classes.append(intent[\"tag\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# words of sentences in patterns\n","words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tags\n","classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#patterns\n","doc_X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# number of tags wrt patterns\n","doc_y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''lemmatizing all the words in the vocab \n","and converting them to lowercase\n","if the words don't appear in punctuation'''\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in punctuation]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# removes punctuation and converts to lowercase\n","words"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1663139471533,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"4FPzehPIgsX3"},"outputs":[],"source":["\n","'''sorting the vocab and classes in alphabetical order \n","and taking the set to ensure no duplicates occur'''\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# removes duplicates and sorts in alphabetical order\n","words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classes"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Creating training and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# list for training data\n","training = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(doc_X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(doc_y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# creating the bag of words model\n","for idx, doc in enumerate(doc_X):\n","    bow = []\n","    text = lemmatizer.lemmatize(doc.lower()) # lemmatizing the text\n","    for word in words:\n","        bow.append(1) if word in text else bow.append(0) # one hot encoding the words if they appear in the text\n","    # marking the index of class that the current pattern is associated to \n","    output_row = [0] * len(classes)\n","    output_row[classes.index(doc_y[idx])] = 1\n","    # adding the one hot encoded BoW and associated classes to training\n","    training.append([bow, output_row])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# shuffling the data and convert it to a numpy array\n","shuffle(training)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","training = array(training, dtype=object)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1663139471533,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"LNSE3v4wg8bT"},"outputs":[],"source":["\n","\n","# splitting the features and target labels\n","train_X = array(list(training[:, 0]))\n","train_y = array(list(training[:, 1]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_y.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"A5GZPEe31Xdy"},"source":["# Building deep learning model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# defining some parameters\n","input_shape = (len(train_X[0]),)\n","output_shape = len(train_y[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# defining the model\n","model = Sequential()\n","model.add(Dense(128, input_shape=input_shape, activation=LeakyReLU(alpha=0.3)))\n","model.add(Dropout(0.25))\n","model.add(Dense(64, activation=LeakyReLU(alpha=0.3)))\n","model.add(Dropout(0.25))\n","model.add(Dense(output_shape, activation=Softmax()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compiling the model\n","adam = Adam(learning_rate=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3686,"status":"ok","timestamp":1663139475207,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"Wd-qsFaxg__L","outputId":"092e6d7d-843c-4ef6-a8f5-6526faf02b41"},"outputs":[],"source":["print(model.summary())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(x=train_X, y=train_y, epochs=200)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f4ye1EP11c8j"},"source":["# Functions for getting results"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1663139475208,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"UMtG7KNuhHFm"},"outputs":[],"source":["# defining a function to clean the text ( tokenizing and then lemmatizing  and returning the tokens array)\n","def clean_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    return tokens\n","\n","# defining a function to get the bag of words(one hot encoding the words if they appear in the text and returning numpy array of bow)\n","def bag_of_words(text, vocab):\n","    tokens = clean_text(text)\n","    bow = [0] * len(vocab)\n","    for w in tokens:\n","        for idx, word in enumerate(vocab):\n","            if word == w:\n","                bow[idx] = 1\n","    return array(bow)\n","\n","# defining a function to predict the class\n","def pred_class(text, vocab, labels):\n","    bow = bag_of_words(text, vocab)\n","    result = model.predict(array([bow]))[0]\n","    thresh = 0.2\n","    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n","    y_pred.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in y_pred:\n","        return_list.append(labels[r[0]])\n","    return return_list\n","\n","# defining a function to get the response (taking the predicted class and returning a random response from the intents.json file)\n","def get_response(intents_list, intents_json):\n","    tag = intents_list[0]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents:\n","        if i[\"tag\"] == tag:\n","            result = choice(i[\"responses\"])\n","            break\n","    return result\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Testing the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","now = datetime.now()\n","print(\"Anuradha is ready to chat! (type 'exit' to quit)\")\n","while True:\n","    try:\n","        message = input(\"You : \")\n","        if message == 'exit':\n","            TEXT = \"Bye! take care\"\n","            print(\"Anuradha :\", TEXT)\n","            break\n","        intents = pred_class(message, words, classes)\n","        result = get_response(intents, data)\n","        if result == \"date\":\n","            TEXT = now.strftime(r'%d/%m/%Y')\n","            print(\"Anuradha :\", TEXT)\n","        elif result == \"time\":\n","            TEXT = now.strftime(r'%H:%M:%S')\n","            print(\"Anuradha :\", TEXT)\n","        else:\n","            print(\"Anuradha :\", result)\n","    except ValueError:\n","        TEXT = \"Sorry, I didn't get that\"\n","        print(\"Anuradha :\", TEXT)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCXzDFtUXVeMQ/Qw0OOsB6","provenance":[]},"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"c1fe09249d939edcb2f8690e36aceef0d23042ced7172f3c00de54e548ae8ce7"}}},"nbformat":4,"nbformat_minor":0}
