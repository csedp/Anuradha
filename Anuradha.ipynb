{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"2wbLfKVB1Qss"},"source":["## Importing the necessary modules"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1663139471533,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"k0U8RAiQgfVD","outputId":"5a65cfb0-232b-46f8-b9d5-1da76b225044"},"outputs":[],"source":["from json import loads\n","from keras import Sequential\n","from keras.layers import Dense, Dropout, LeakyReLU, Softmax\n","from keras.optimizers import Adam\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from numpy import array\n","from random import choice, shuffle\n","from string import punctuation\n","'''\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')\n","'''\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = loads(open(\"dataset/intents.json\").read())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tB3oHTpr1Uz_"},"source":["## Using NLP to clean the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","\n","# Each list to create\n","words = [] # tokenized words of sentences in patterns\n","classes = [] # tags\n","doc_X = [] # patterns\n","doc_y = [] # tags ocurring number of times wrt patterns\n","\n","'''Looping through all the intents and tokenizing each patterns\n","and appending tokens to words, the patterns \n","and the associated tag to their associated list'''\n","for intent in data[\"intents\"]:\n","    for pattern in intent[\"patterns\"]:\n","        tokens = word_tokenize(pattern)\n","        words.extend(tokens)\n","        doc_X.append(pattern)\n","        doc_y.append(intent[\"tag\"])\n","    # add the tag to the classes if it's not there already\n","    if intent[\"tag\"] not in classes:\n","        classes.append(intent[\"tag\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''lemmatizing all the words in the vocab \n","and converting them to lowercase\n","if the words don't appear in punctuation'''\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in punctuation] # removes punctuation and converts to lowercase\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1663139471533,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"4FPzehPIgsX3"},"outputs":[],"source":["'''sorting the vocab and classes in alphabetical order \n","and taking the set to ensure no duplicates occur'''\n","# removes duplicates and sorts in alphabetical order\n","words = sorted(set(words)) \n","classes = sorted(set(classes))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"lenght of words: \", len(words))\n","print(\"length of doc_X\", len(doc_X))\n","print(\"length of doc_y\", len(doc_y))\n","print(\"length of classes\", len(classes))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Creating training and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# list for training data\n","training = []\n","\n","# creating the bag of words model\n","for idx, doc in enumerate(doc_X):\n","    bow = []\n","    text = lemmatizer.lemmatize(doc.lower()) # lemmatizing the sentence and converting to lowercase\n","    for word in words:\n","        bow.append(1) if word in text else bow.append(0) # one hot encoding the words if they appear in the text\n","    output_row = [0] * len(classes)\n","    # marking the index of class that the current pattern is associated to as 1\n","    output_row[classes.index(doc_y[idx])] = 1\n","    # adding the one hot encoded BoW and associated classes to training\n","    training.append([bow, output_row])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# shuffling the data and convert it to a numpy array\n","shuffle(training)\n","training = array(training, dtype=object)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1663139471533,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"LNSE3v4wg8bT"},"outputs":[],"source":["# splitting the features and target labels\n","train_X = array(list(training[:, 0])) # features\n","train_y = array(list(training[:, 1])) # target labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_X\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_y\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Shape of train_X: {train_X.shape}\")\n","print(f\"Shape of train_y: {train_y.shape}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"A5GZPEe31Xdy"},"source":["# Building deep learning model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# defining some parameters\n","input_shape = (len(train_X[0]),)\n","output_shape = len(train_y[0])\n","print(\"input_shape: \", input_shape)\n","print(\"output_shape: \", output_shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# defining the model\n","model = Sequential()\n","model.add(Dense(128, input_shape=input_shape, activation=LeakyReLU(alpha=0.3)))\n","model.add(Dropout(0.25))\n","model.add(Dense(64, activation=LeakyReLU(alpha=0.3)))\n","model.add(Dropout(0.25))\n","model.add(Dense(output_shape, activation=Softmax()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compiling the model\n","adam = Adam(learning_rate=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3686,"status":"ok","timestamp":1663139475207,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"Wd-qsFaxg__L","outputId":"092e6d7d-843c-4ef6-a8f5-6526faf02b41"},"outputs":[],"source":["print(model.summary())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(x=train_X, y=train_y, epochs=200)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f4ye1EP11c8j"},"source":["# Functions for getting results"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1663139475208,"user":{"displayName":"Debjit Pal","userId":"00041353721650788544"},"user_tz":-330},"id":"UMtG7KNuhHFm"},"outputs":[],"source":["# defining a function to clean the text\n","# tokenizing and then lemmatizing and returning the tokens array\n","def clean_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    return tokens\n","\n","# defining a function to get the bag of words\n","# one hot encoding the words if they appear in the text and returning numpy array of bow\n","def bag_of_words(text, vocab):\n","    tokens = clean_text(text)\n","    bow = [0] * len(vocab)\n","    for w in tokens:\n","        for idx, word in enumerate(vocab):\n","            if word == w:\n","                bow[idx] = 1\n","    return array(bow)\n","\n","# defining a function to predict the class\n","def pred_class(text, vocab, labels):\n","    bow = bag_of_words(text, vocab)\n","    result = model.predict(array([bow]))[0]\n","    thresh = 0.2\n","    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n","    y_pred.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in y_pred:\n","        return_list.append(labels[r[0]])\n","    return return_list\n","\n","# defining a function to get the response \n","# taking the predicted class and returning a random response from the intents.json file\n","def get_response(intents_list, intents_json):\n","    tag = intents_list[0]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents:\n","        if i[\"tag\"] == tag:\n","            result = choice(i[\"responses\"])\n","            break\n","    return result\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Testing the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","import platform\n","import pyttsx3\n","import speech_recognition as sr\n","\n","now = datetime.now()\n","engine = pyttsx3.init()\n","r = sr.Recognizer()\n","mic = sr.Microphone()\n","\n","if platform.system() == \"Windows\":  # for Windows\n","    engine.setProperty(\n","        'voice', r\"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\")\n","elif platform.system() == \"Darwin\":  # for macOS\n","    engine.setProperty(\n","        'voice', \"com.apple.speech.synthesis.voice.samantha\")\n","else:\n","    pass\n","\n","TEXT = \"Anuradha is ready to chat! (type 'exit' to quit)\"\n","print(TEXT)\n","engine.say(TEXT)\n","engine.runAndWait()\n","\n","while True:\n","    with mic as source:\n","        r.adjust_for_ambient_noise(source)\n","        r.dynamic_energy_threshold = True\n","        print(\"Say Now\")\n","        audio = r.record(source, duration=3)\n","    try:\n","        message = r.recognize_google(audio, language=\"en-IN\") # message = input(\"You : \")\n","        intents = pred_class(message, words, classes)\n","        result = get_response(intents, data)\n","        if message == 'exit':\n","            TEXT = \"Bye! take care\"\n","            break\n","        elif result == \"date\":\n","            TEXT = now.strftime(r'%d/%m/%Y')\n","        elif result == \"time\":\n","            TEXT = now.strftime(r'%H:%M:%S')\n","        else:\n","            TEXT = result\n","    except sr.UnknownValueError:\n","        TEXT = \"Sorry, I didn't get that\"\n","    except sr.RequestError as e:\n","        TEXT = \"Sorry, can't connect to the service\"\n","    finally:\n","        try:\n","            print(\"You :\", message)\n","        except NameError:\n","            pass\n","        finally:\n","            print(\"Anuradha :\", TEXT)\n","            engine.say(TEXT)\n","            engine.runAndWait()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCXzDFtUXVeMQ/Qw0OOsB6","provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"c1fe09249d939edcb2f8690e36aceef0d23042ced7172f3c00de54e548ae8ce7"}}},"nbformat":4,"nbformat_minor":0}
